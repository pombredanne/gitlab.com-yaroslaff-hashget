#!/usr/bin/python3

import argparse
import os
import urllib.parse
import requests
import json
import hashlib
import re
import sys
import time
import logging
import tempfile
import hashlib
import shutil

from requests.packages.urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
    

#from debsnapm import load_release

import hashget
from hashget.utils import kmgt

BUF_SIZE = 1024*1024

repo = dict()
log=None
hashdb = None

def download_file(url, prefix="/tmp", headers=None):
    headers = headers or dict()
    out = dict()

    chunk_size = 1024*1024
    basename = url.split('/')[-1]
    local_filename = os.path.join(prefix, basename)

    r = requests.get(url, stream=True, headers=headers)
    
    if r.status_code != 200:
        return None
        
    with open(local_filename, 'wb') as f:
        for chunk in r.iter_content(chunk_size=chunk_size): 
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)

    return local_filename



def deb2url(root, project, basename):
    if basename.startswith('lib'):
        namedir = os.path.join(project, 'name', basename[0:3], basename[3], basename[4])
    else:
        namedir = os.path.join(project, 'name', basename[0], basename[1])

    namepath = os.path.join(namedir, basename)
    project_url = urllib.parse.urljoin(root, project)
    file_url = urllib.parse.urljoin(project_url, namepath)
    return file_url



def fetch(hashserver, project, deb):
    debinfo_url = deb2url(hashserver, project, deb)
    r = requests.get(debinfo_url)
    debinfo = json.loads(r.text)
    
    urls = list()

    
    if debinfo['repo'] in repo:
        urls.append(urllib.parse.urljoin(repo[debinfo['repo']], debinfo['urlpath']))
    
    urls.append(debinfo['url'])
    urls.append(debinfo['snap_url'])
    
    for url in urls:
        f = download_file(url, prefix=".")
        if f:    
            print("Downloaded {} to {}".format(url, f))
            return
        else:
            print("Failed to get from {}".format(url))
    

#
# Prepare
#

def prepare(root, hashdb, anchsz, filesz, skipdirs, excludefile):
    """
        ANCHORS ??? do we need it here? maybe delete?
    """

    files = prepare_readfiles(root, anchsz, filesz, skipdirs)
    log.debug('files are ready')
    ph = dict() # package hashes to URLs
    rfile = hashget.restorefile.restorefile()
    
    with open(excludefile,'w') as excf:
        for f in files:
            try:
                """
                    save url and package hashes, then write to snf
                    write file info to snf
                """
                hashspec = f.get_hashspec()
                phash = hashdb.fhash2phash(hashspec)
                ph[phash] = hashdb.phash2url(phash)
                excf.write("{}\n".format(os.path.relpath(f.filename, root)))    
                rfile.add_file(f)
                    
            except KeyError:
                # unique file, not found in any packages
                pass
                
        for ph, purl in ph.items():
            rfile.add_package(url = purl, hashspec = ph)
        
        rfile.save(os.path.join(root,'.hashget-restore'))

    # guess_packages(root, files)



def prepare_readfiles(root, anchsz, filesz, skipdirs):

    def skipdir(d, skipdirs):
        # maybe skip it?
        for sd in skipdirs:
            if (d+'/').startswith(sd):
                return True
        return False

    total = 0
    files = list()

    for directory, subdirs, dirfiles in os.walk(root):
    
        if skipdir(directory, skipdirs):
            continue
                
    
        for basename in dirfiles:
            total += 1        
            path = os.path.join(directory, basename)                        
            
            if os.path.islink(path) or not os.path.isfile(path):
                continue
            
            f = hashget.File(path, root=root)
            if f.size > filesz:
                files.append(f)
            #if f.size > 100*1024:
            #    anchors.append(f)

    # sort anchors
    files = hashget.FileList(sorted(files, key = lambda k: getattr(k,'size'), reverse=True))
    
    return files
    

#
# PostUnpack
#
def postunpack(root, hashdb):
    cg = hashget.CacheGet()

    rfile = hashget.restorefile.restorefile(os.path.join(root, '.hashget-restore'))
    rfile.preiteration()
    
    stat_cached = 0
    stat_downloaded = 0
    stat_recovered = 0
    stat_files = 0
    started = time.time()
    
    for purl in rfile.packages():
        #pbasename = purl.split('/')[-1]
        #ptdir = os.path.join(tdir, pbasename)
        #os.mkdir(ptdir)

        # lp = cg.get(purl)
        
        log.debug('restore from URL ' + purl)
        p = hashget.package.Package(url = purl)
        p.download()
        p.hash_content()
        
        for pf in p.files:
            hashspec = pf.get_hashspec()
            try:
                rf = rfile.fbyhash(hashspec)
            except LookupError:
                pass
            else:
                print('recovered ' + rf.filename)
                rf.recover(pf.filename)
                stat_recovered += rf.size
                stat_files += 1
                
        stat_cached += p.stat_cached
        stat_downloaded += p.stat_downloaded
        p.cleanup()                        
    
    log.debug('Recovered {} files {} bytes ({} downloaded, {} cached) in {:.2f}s'.format(
            stat_files,
            kmgt(stat_recovered),
            kmgt(stat_downloaded),
            kmgt(stat_cached),
            time.time() - started
        ))
    
    # delete tmpdir

#
# Crawling
#

def crawl1(p):
    
   
    print("Crawl", p)
    url = p.snapshot_url()    
    
    cg = hashget.CacheGet()
    
    get_started = time.time()
    f = cg.get(url)

    print(f['code'], kmgt(f['size']), f['url'])

    #print json.dumps(f, indent=4)
     

    # calc file hashes
    fhashes = hashget.File(f['file'])
    # print "FILE:", fhashes

    attrs = dict() 
    #{
    #    'sha256': fhashes.hashes.sha256,
    #    'md5': fhashes.hashes.md5
    #    }

    anchors, files = hashget.walk_arc(f['file'], minsz = 102400, log=log)
    
    
    hp = hashget.HashPackage(
            anchors = anchors,
            files = files,
            url = url,
            attrs = attrs,
            sha256 = fhashes.hashes.sha256,
            md5 = fhashes.hashes.md5,
            signature = p.get_signature()
        )
    
    print("took: {:.2f}".format( time.time() - get_started ))

    hashdb.submit(hp)

def debcrawl_packages(root, delay=0):

    cnt_total = 0
    cnt_installed = 0
    cnt_not_installed = 0
    cnt_already = 0
    cnt_new = 0
    
    started = time.time()

    def file2pkgname(filename):
        # delete suffix (.list) or :arch.list
        if ':' in filename:
            return filename.split(':')[0]
        else:
            return '.'.join(filename.split('.')[:-1])
        

    status = hashget.debian.load_release(os.path.join(root, 'var/lib/dpkg/status'))
    
    for pdict in status:
        
        p = hashget.debian.DebPackage(info = pdict)

        cnt_total += 1 
    
        if not p.is_installed():
            cnt_installed += 1
            continue        

        cnt_not_installed += 1
        
    
        if hashdb.sig_present(p.get_signature()):
            cnt_already += 1
            continue
        
        # print "==="
        # print json.dumps(p, indent=4)
        # print "---"
        crawl1(p)
        cnt_new += 1
        time.sleep(delay)
        
    print("Crawling done ({:.2f})".format(time.time() - started))
    print("total: {} installed: {}, not installed: {}, already in hashdb: {}, new: {}".format(
            cnt_total, cnt_installed, cnt_not_installed, cnt_already, cnt_new
        ))


def get(hashspec, outdir):

    url = hashdb.hash2url(hashspec)

    p = hashget.package.Package(url = url, log=log)
    p.download()
    if p.hashes.match_hashspec(hashspec):
        dst = os.path.join(outdir, p.basename)
        shutil.copy(p.path, dst)
        log.info(dst)
        return dst

    log.debug('package not matches hashspec {}'.format(hashspec))
    
    src = p.hash2path(hashspec)    
    dst = os.path.join(outdir, os.path.basename(src))
    shutil.copy(src, dst)
    p.cleanup() 
    log.info(dst)
    
    
        
def main():

    global log, hashdb

    def_anchsz = 100*1024
    def_filesz = 1024
    def_hashserver = 'http://hashget.okerr.com/'
    def_project = 'DEBSNAP'
    def_excludefile = os.path.expanduser("~/hashget-exclude") 
    def_skipdirs = ['var/cache/','var/lib/apt/']
    def_sleep = 2
    def_outdir = '.'

    parser = argparse.ArgumentParser(description='HashGet fetcher and deduplicator')

    g = parser.add_argument_group('Commands')
    g.add_argument('--get', default=None, metavar='HASHSPEC', help='get file by hash')
    g.add_argument('--prepack', '-p', default=None, metavar='DIR', help='prepare DIR for hash-tarring')
    g.add_argument('--postunpack', '-u', default=None, metavar='DIR', help='post-unpack')
    g.add_argument('--fetch', default=None, help='BROKEN? fetch .deb file by name')


    g = parser.add_argument_group('Local HashDB commands')
    g.add_argument('--debcrawl', default=None, metavar='DIR', help='snapshot crawl packages in DIR')
    g.add_argument('--submit', default=None, metavar='URL', help='submit URL')

    g = parser.add_argument_group('Options')
    g.add_argument('--anchsz', type=int, default=def_anchsz, help='min size of anchors ({})'.format(def_anchsz))
    g.add_argument('--filesz', type=int, default=def_filesz, help='min size of files')
    g.add_argument('--hashserver', default=def_hashserver, help='hashserver URL ({})'.format(def_hashserver))
    g.add_argument('--project', default=def_project, help='project name ({})'.format(def_project))
    g.add_argument('--repo', nargs=2, action='append', metavar=('REPO_TAG', 'REPO_URL'), default=list(), help='repositories (many)')
    g.add_argument('-X','--exclude-from', metavar='FILENAME', dest='excludefile', default=def_excludefile, help='Exclude file (for -X tar option) ({})'.format(def_excludefile))
    g.add_argument('--sleep', type=int, default=def_sleep, help='delay ({}s)'.format(def_sleep))
    g.add_argument('--outdir', '-o', default=def_outdir, help='dir where to store --get files ({})'.format(def_outdir))
    g.add_argument('--etag', default=False, action='store_true', help='verify HTTP E-Tag when reuse cached files')

    g = parser.add_argument_group('Target system specification')
    g.add_argument('--skip', nargs='*', default=def_skipdirs, help='Do not try to dedup these dirs ({}). Relative to --prepare path'.format(def_skipdirs))


    g = parser.add_argument_group('Logging, output')
    g.add_argument('--logfile', default=None, help='log file name')
    g.add_argument('-v', dest='verbose', default=False, action='store_true', help='verbose mode')
    g.add_argument('-q', dest='quiet', default=False, action='store_true', help='quiet mode')


    args = parser.parse_args()

    hashdb = hashget.DirHashDB()

    # configure logging
    if args.verbose:
        loglevel = logging.DEBUG
    else:
        loglevel = logging.INFO

    if args.quiet:
        loglevel = logging.ERROR

    hashget.cacheget.opt_verify_etag = args.etag

    # logging.basicConfig(level = loglevel, format='%(asctime)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    logging.basicConfig(level = loglevel, format='%(message)s')
    log = logging.getLogger('slowget')
    if args.logfile:
        fh = logging.FileHandler(args.logfile)
        fh.setFormatter(logging.Formatter(FORMAT, datefmt='%Y-%m-%d %H:%M:%S'))
        log.addHandler(fh)


    for e in os.environ:        
        if e.startswith("REPO_"):
            rname = e[len("REPO_"):]
            repo[rname] = os.environ[e]
            print("import repo {} {} from env".format(rname, repo[rname]))

    for r in args.repo:
        repo[r[0]]=r[1]
        print("import repo {} {} from args".format(rname, repo[rname]))
        
    if args.debcrawl:
        debcrawl_packages(args.crawl, args.sleep)

    if args.prepack:
        
        skipdirs = [ os.path.join(args.prepack, d) for d in args.skip ]    
                          
        prepare(args.prepack,
            hashdb = hashdb,
            anchsz = args.anchsz,
            filesz = args.filesz,
            skipdirs = skipdirs,
            excludefile = args.excludefile
            )

    if args.postunpack:
        postunpack(args.postunpack, hashdb = hashdb)
            
    if args.fetch:
        fetch(args.hashserver, args.project, args.fetch)

    if args.get:
        get(args.get, args.outdir)

main()



