#!/usr/bin/python3

import argparse
import os
import urllib.parse
import requests
import json
import hashlib
import re
import sys
import time
import logging
import tempfile
import hashlib

from requests.packages.urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
    

#from debsnapm import load_release

import hashget
from hashget.utils import kmgt

BUF_SIZE = 1024*1024

repo = dict()
log=None
hashdb = None

def download_file(url, prefix="/tmp", headers=None):
    headers = headers or dict()
    out = dict()

    chunk_size = 1024*1024
    basename = url.split('/')[-1]
    local_filename = os.path.join(prefix, basename)

    r = requests.get(url, stream=True, headers=headers)
    
    if r.status_code != 200:
        return None
        
    with open(local_filename, 'wb') as f:
        for chunk in r.iter_content(chunk_size=chunk_size): 
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)

    return local_filename



def deb2url(root, project, basename):
    if basename.startswith('lib'):
        namedir = os.path.join(project, 'name', basename[0:3], basename[3], basename[4])
    else:
        namedir = os.path.join(project, 'name', basename[0], basename[1])

    namepath = os.path.join(namedir, basename)
    project_url = urllib.parse.urljoin(root, project)
    file_url = urllib.parse.urljoin(project_url, namepath)
    return file_url



def fetch(hashserver, project, deb):
    debinfo_url = deb2url(hashserver, project, deb)
    r = requests.get(debinfo_url)
    debinfo = json.loads(r.text)
    
    urls = list()

    
    if debinfo['repo'] in repo:
        urls.append(urllib.parse.urljoin(repo[debinfo['repo']], debinfo['urlpath']))
    
    urls.append(debinfo['url'])
    urls.append(debinfo['snap_url'])
    
    for url in urls:
        f = download_file(url, prefix=".")
        if f:    
            print("Downloaded {} to {}".format(url, f))
            return
        else:
            print("Failed to get from {}".format(url))
    

#
# Prepare
#

def prepare(root, hashdb, anchsz, filesz, skipdirs, excludefile):
    """
        ANCHORS ???
    """

    files = prepare_readfiles(root, anchsz, filesz, skipdirs)
    ph = dict() # package hashes to URLs
    hlist = hashget.hashlist.hashlist()
    
    with open(excludefile,'w') as excf:
        for f in files:
            try:
                """
                    save url and package hashes, then write to snf
                    write file info to snf
                """
                phash = hashdb.fhash2phash(f.hashes.sha256)
                ph[phash] = hashdb.phash2url(phash)
                excf.write("{}\n".format(os.path.relpath(f.filename, root)))    
                hlist.add_file(f)
                    
            except KeyError:
                # unique file, not found in any packages
                pass
                
        for ph, purl in ph.items():
            snap.add_package(url = purl, sha256 = ph)
        
        hlist.save(os.path.join(root,'.hashlist'))

    # guess_packages(root, files)



def prepare_readfiles(root, anchsz, filesz, skipdirs):

    def skipdir(d, skipdirs):
        # maybe skip it?
        for sd in skipdirs:
            if (d+'/').startswith(sd):
                return True
        return False

    total = 0
    files = list()

    for directory, subdirs, dirfiles in os.walk(root):
    
        if skipdir(directory, skipdirs):
            continue
                
    
        for basename in dirfiles:
            total += 1        
            path = os.path.join(directory, basename)                        
            
            if os.path.islink(path) or not os.path.isfile(path):
                continue
            
            f = hashget.File(path, root=root)
            if f.size > filesz:
                files.append(f)
            #if f.size > 100*1024:
            #    anchors.append(f)

    # sort anchors
    files = hashget.FileList(sorted(files, key = lambda k: getattr(k,'size'), reverse=True))
    
    return files
    

#
# PostUnpack
#
def postunpack(root, hashdb):
    print("postunpack: " + root)
    cg = hashget.CacheGet()

    snap = hashget.snapfile.snapfile(os.path.join(root, '.hashist'))
    snap.preiteration()
    print(snap)
    
    tdir = tempfile.mkdtemp(prefix='hashget-postunpack-')
    
    for purl in snap.packages():
        pbasename = purl.split('/')[-1]
        ptdir = os.path.join(tdir, pbasename)
        os.mkdir(ptdir)

        lp = cg.get(purl)
        
        hashget.unpack_deb(lp['file'], ptdir)
    
        for r, dirs, files in os.walk(ptdir):
            for basename in files:
                        
                fpath = os.path.join(r, basename)
                if not os.path.isfile(fpath) or os.path.islink(fpath):
                    continue

                h = hashlib.sha256()
                
                with open(fpath, 'rb') as f:
                    while True:
                        data = f.read(BUF_SIZE)
                        if not data:
                            break
                        h.update(data)
                
                sha256 = h.hexdigest()            

                try:
                    f = snap.fbyhash(sha256)
                    f.recover(fpath)
                    print(f.filename)
                    # snap.set_processed(sha256)
                except LookupError as e:
                    # print "not found: {}".format(e)
                    continue
    
    for f in snap.files():
        # print "look for", f.hashes.sha256
        phash = hashdb.fhash2phash(f.hashes.sha256)
        # print phash
    
        # get from hashdb
        
    # delete tmpdir

#
# Crawling
#

def crawl1(p):
    
   
    print("Crawl", p)
    url = p.snapshot_url()    
    
    cg = hashget.CacheGet()
    
    get_started = time.time()
    f = cg.get(url)

    print(f['code'], kmgt(f['size']), f['url'])

    #print json.dumps(f, indent=4)
     

    # calc file hashes
    fhashes = hashget.File(f['file'])
    # print "FILE:", fhashes

    attrs = dict() 
    #{
    #    'sha256': fhashes.hashes.sha256,
    #    'md5': fhashes.hashes.md5
    #    }

    anchors, files = hashget.walk_arc(f['file'], minsz = 102400, log=log)
    
    
    hp = hashget.HashPackage(
            anchors = anchors,
            files = files,
            url = url,
            attrs = attrs,
            sha256 = fhashes.hashes.sha256,
            md5 = fhashes.hashes.md5,
            signature = p.get_signature()
        )
    
    print("took: {:.2f}".format( time.time() - get_started ))

    hashdb.submit(hp)

def debcrawl_packages(root, delay=0):

    cnt_total = 0
    cnt_installed = 0
    cnt_not_installed = 0
    cnt_already = 0
    cnt_new = 0
    
    started = time.time()

    def file2pkgname(filename):
        # delete suffix (.list) or :arch.list
        if ':' in filename:
            return filename.split(':')[0]
        else:
            return '.'.join(filename.split('.')[:-1])
        

    status = hashget.debian.load_release(os.path.join(root, 'var/lib/dpkg/status'))
    
    for pdict in status:
        
        p = hashget.debian.DebPackage(info = pdict)

        cnt_total += 1 
    
        if not p.is_installed():
            cnt_installed += 1
            continue        

        cnt_not_installed += 1
        
    
        if hashdb.sig_present(p.get_signature()):
            cnt_already += 1
            continue
        
        # print "==="
        # print json.dumps(p, indent=4)
        # print "---"
        crawl1(p)
        cnt_new += 1
        time.sleep(delay)
        
    print("Crawling done ({:.2f})".format(time.time() - started))
    print("total: {} installed: {}, not installed: {}, already in hashdb: {}, new: {}".format(
            cnt_total, cnt_installed, cnt_not_installed, cnt_already, cnt_new
        ))


def get(hashspec):

    url = hashdb.hash2url(hashspec)

    p = hashget.package.Package(url = url)
    print(p)
    
        
def main():

    global log, hashdb

    def_anchsz = 100*1024
    def_filesz = 1024
    def_hashserver = 'http://hashget.okerr.com/'
    def_project = 'DEBSNAP'
    def_excludefile = os.path.expanduser("~/hashget-exclude") 
    def_skipdirs = ['var/cache/','var/lib/apt/']
    def_sleep = 2

    parser = argparse.ArgumentParser(description='HashGet fetcher and deduplicator')

    g = parser.add_argument_group('Commands')
    g.add_argument('--get', default=None, metavar='HASHSPEC', help='get file by hash')
    g.add_argument('--prepack', '-p', default=None, metavar='DIR', help='prepare DIR for hash-tarring')
    g.add_argument('--postunpack', '-u', default=None, metavar='DIR', help='post-unpack')
    g.add_argument('--fetch', default=None, help='BROKEN? fetch .deb file by name')


    g = parser.add_argument_group('Local HashDB commands')
    g.add_argument('--debcrawl', default=None, metavar='DIR', help='snapshot crawl packages in DIR')
    g.add_argument('--submit', default=None, metavar='URL', help='submit URL')

    g = parser.add_argument_group('Options')
    g.add_argument('--anchsz', type=int, default=def_anchsz, help='min size of anchors ({})'.format(def_anchsz))
    g.add_argument('--filesz', type=int, default=def_filesz, help='min size of files')
    g.add_argument('--hashserver', default=def_hashserver, help='hashserver URL ({})'.format(def_hashserver))
    g.add_argument('--project', default=def_project, help='project name ({})'.format(def_project))
    g.add_argument('--repo', nargs=2, action='append', metavar=('REPO_TAG', 'REPO_URL'), default=list(), help='repositories (many)')
    g.add_argument('-X','--exclude-from', metavar='FILENAME', dest='excludefile', default=def_excludefile, help='Exclude file (for -X tar option) ({})'.format(def_excludefile))
    g.add_argument('--sleep', type=int, default=def_sleep, help='delay ({}s)'.format(def_sleep))


    g = parser.add_argument_group('Target system specification')
    g.add_argument('--skip', nargs='*', default=def_skipdirs, help='Do not try to dedup these dirs ({}). Relative to --prepare path'.format(def_skipdirs))


    g = parser.add_argument_group('Logging, output')
    g.add_argument('--logfile', default=None, help='log file name')
    g.add_argument('-v', dest='verbose', default=False, action='store_true', help='verbose mode')
    g.add_argument('-q', dest='quiet', default=False, action='store_true', help='quiet mode')


    args = parser.parse_args()

    hashdb = hashget.DirHashDB()

    # configure logging
    FORMAT = '%(asctime)s %(message)s'
    if args.verbose:
        loglevel = logging.DEBUG
    else:
        loglevel = logging.INFO

    if args.quiet:
        loglevel = logging.ERROR

    logging.basicConfig(level = loglevel, format='%(asctime)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    log = logging.getLogger('slowget')
    if args.logfile:
        fh = logging.FileHandler(args.logfile)
        fh.setFormatter(logging.Formatter(FORMAT, datefmt='%Y-%m-%d %H:%M:%S'))
        log.addHandler(fh)



    for e in os.environ:        
        if e.startswith("REPO_"):
            rname = e[len("REPO_"):]
            repo[rname] = os.environ[e]
            print("import repo {} {} from env".format(rname, repo[rname]))

    for r in args.repo:
        repo[r[0]]=r[1]
        print("import repo {} {} from args".format(rname, repo[rname]))
        
    if args.debcrawl:
        debcrawl_packages(args.crawl, args.sleep)

    if args.prepack:
        
        skipdirs = [ os.path.join(args.prepack, d) for d in args.skip ]    
                          
        prepare(args.prepack,
            hashdb = hashdb,
            anchsz = args.anchsz,
            filesz = args.filesz,
            skipdirs = skipdirs,
            excludefile = args.excludefile
            )

    if args.postunpack:
        postunpack(args.postunpack, hashdb = hashdb)
            
    if args.fetch:
        fetch(args.hashserver, args.project, args.fetch)

    if args.get:
        print("get", args.get)
        get(args.get)

main()



